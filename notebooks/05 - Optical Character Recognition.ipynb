{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reconocimiento óptico de caracteres\r\n",
        "\r\n",
        "![Un robot leyendo un periódico](./images/ocr.jpg)\r\n",
        "\r\n",
        "Uno de los desafíos más habituales de Computer Vision es detectar e interpretar el texto de una imagen. Este tipo de procesamiento suele conocerse como *reconocimiento óptico de caracteres* (OCR)\r\n",
        "\r\n",
        "## Usar el servicio Computer Vision para encontrar el texto de una imagen\r\n",
        "\r\n",
        "El servicio **Computer Vision** de Cognitive Services permite realizar tareas de OCR, como estas:\r\n",
        "\r\n",
        "- Una API de **OCR** que puede usar para leer el texto en varios idiomas. La API se puede usar de forma sincrónica y es muy útil cuando se necesita detectar y leer una pequeña cantidad de texto de una imagen.\r\n",
        "- Una **Read** API optimizada para documentos más grandes. Esta API se usa de forma asincrónica y se puede utilizar para texto impreso y escrito a mano.\r\n",
        "\r\n",
        "Para usar este servicio, cree un recurso de **Computer Vision** o un recurso de **Cognitive Services**.\r\n",
        "\r\n",
        "Si no lo ha hecho aún, cree un recurso de **Cognitive Services** en su suscripción de Azure.\r\n",
        "\r\n",
        "> **Nota**: Si ya tiene un recurso de Cognitive Services, abra su página de **Inicio rápido** en Azure Portal y copie la clave y el punto de conexión en la siguiente celda. En caso contrario, siga estos pasos para crear uno.\r\n",
        "\r\n",
        "1. En la pestaña de otro explorador, abra Azure Portal (https://portal.azure.com) e inicie sesión con su cuenta de Microsoft.\r\n",
        "\r\n",
        "2. Haga clic en el botón **&#65291;Crear un recurso**, busque *Cognitive Services* y cree un recurso de **Cognitive Services** con esta configuración:\r\n",
        "    - **Suscripción**: *su suscripción de Azure*.\r\n",
        "    - **Grupo de recursos**: *seleccione o cree un grupo de recursos con un nombre único.*\r\n",
        "    - **Región**: *seleccione cualquier región disponible*:\r\n",
        "    - **Nombre**: *escriba un nombre único*.\r\n",
        "    - **Plan de tarifa**: S0\r\n",
        "    - **Confirmo que he leído y comprendido las notificaciones**: seleccionado.\r\n",
        "3. Espere a que la implementación finalice. Vaya al recurso de Cognitive Services y, en la página **Información general**, haga clic en el vínculo para administrar las claves del servicio. Necesitará el punto de conexión y las claves para conectarse a su recurso de Cognitive Services desde aplicaciones de cliente.\r\n",
        "\r\n",
        "### Obtener la clave y el punto de conexión de un recurso de Cognitive Services\r\n",
        "\r\n",
        "Para usar su recurso de Cognitive Services, las aplicaciones de cliente necesitan su clave de autenticación y su punto de conexión:\r\n",
        "\r\n",
        "1. En Azure Portal, en la página **Claves y punto de conexión** de su recurso de Cognitive Services, copie la **Key1** de su recurso y péguela en el siguiente código, en sustitución de **YOUR_COG_KEY**.\r\n",
        "2. Copie el **Punto de conexión** de su recurso y péguelo en el siguiente código, en sustitución de **YOUR_COG_ENDPOINT**.\r\n",
        "3. Haga clic en **Run cell** (&#9655;), a la izquierda de la celda siguiente, para ejecutar su código."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "cog_key = 'YOUR_COG_KEY'\n",
        "cog_endpoint = 'YOUR_COG_ENDPOINT'\n",
        "\n",
        "print('Ready to use cognitive services at {} using key {}'.format(cog_endpoint, cog_key))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599694246277
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora que ha configurado la clave y el punto de conexión, puede usar su recurso de Computer Vision para extraer el texto de una imagen.\r\n",
        "\r\n",
        "Empezaremos con la API de **OCR**, que permite analizar de forma sincrónica una imagen y leer el texto que contiene. En este caso, tenemos una imagen publicitaria de la empresa ficticia Northwind Traders que incluye algo de texto. Ejecute la celda siguiente para leerlo. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageDraw\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# Obtener un cliente del servicio Computer Vision\r\n",
        "computervision_client = ComputerVisionClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n",
        "\n",
        "# Lectura del archivo de imagen\r\n",
        "image_path = os.path.join('data', 'ocr', 'advert.jpg')\n",
        "image_stream = open(image_path, \"rb\")\n",
        "\n",
        "# Usar el servicio Computer Vision para encontrar el texto de la imagen\r\n",
        "read_results = computervision_client.recognize_printed_text_in_stream(image_stream)\n",
        "\n",
        "# Procesar el texto línea a línea\r\n",
        "for region in read_results.regions:\n",
        "    for line in region.lines:\n",
        "\n",
        "        # Lea las palabras en la línea de texto\n",
        "        line_text = ''\n",
        "        for word in line.words:\n",
        "            line_text += word.text + ' '\n",
        "        print(line_text.rstrip())\n",
        "\n",
        "# Abrir la imagen para mostrarla.\r\n",
        "fig = plt.figure(figsize=(7, 7))\n",
        "img = Image.open(image_path)\n",
        "draw = ImageDraw.Draw(img)\n",
        "plt.axis('off')\n",
        "plt.imshow(img)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599694257280
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El texto de la imagen está organizado en una estructura jerárquica de regiones, líneas y palabras. El código lee esta estructura para obtener los resultados.\r\n",
        "\r\n",
        "En los resultados, podemos ver el texto leído encima de la imagen. \r\n",
        "\r\n",
        "## Mostrar cuadros de límite\r\n",
        "\r\n",
        "Los resultados también incluyen las coordenadas del *cuadro de límite* de las líneas de texto y las palabras individuales encontradas en la imagen. Ejecute la siguiente celda para ver los cuadros de límite de las líneas de texto de la imagen publicitaria utilizada anteriormente."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Abrir la imagen para mostrarla.\r\n",
        "fig = plt.figure(figsize=(7, 7))\n",
        "img = Image.open(image_path)\n",
        "draw = ImageDraw.Draw(img)\n",
        "\n",
        "# Procesar el texto línea a línea\r\n",
        "for region in read_results.regions:\n",
        "    for line in region.lines:\n",
        "\n",
        "        # Muestre la posición de la línea de texto\n",
        "        l,t,w,h = list(map(int, line.bounding_box.split(',')))\n",
        "        draw.rectangle(((l,t), (l+w, t+h)), outline='magenta', width=5)\n",
        "\n",
        "        # Lea las palabras en la línea de texto\n",
        "        line_text = ''\n",
        "        for word in line.words:\n",
        "            line_text += word.text + ' '\n",
        "        print(line_text.rstrip())\n",
        "\n",
        "# Mostrar la imagen con la ubicación del texto resaltada\r\n",
        "plt.axis('off')\n",
        "plt.imshow(img)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599694266106
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el resultado, el cuadro de límite de cada línea de texto aparece como un rectángulo en la imagen.\r\n",
        "\r\n",
        "## Usar la Read API\r\n",
        "\r\n",
        "La API de OCR utilizada anteriormente es útil para imágenes con poco texto. Cuando se necesita leer una cantidad mayor de texto, como pasa con los documentos escaneados, puede usar la **Read** API. Se trata de un proceso compuesto por varios pasos:\r\n",
        "\r\n",
        "1. Envíe una imagen al servicio Computer Vision para leerla y analizarla de forma asincrónica.\r\n",
        "2. Espere a que se complete la operación de análisis.\r\n",
        "3. Recupere los resultados del análisis.\r\n",
        "\r\n",
        "Ejecute la siguiente celda para usar este proceso y leer el texto de una carta escaneada dirigida al director de una tienda de Northwind Traders."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
        "from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import time\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# Lectura del archivo de imagen\r\n",
        "image_path = os.path.join('data', 'ocr', 'letter.jpg')\n",
        "image_stream = open(image_path, \"rb\")\n",
        "\n",
        "# Obtener un cliente del servicio Computer Vision\r\n",
        "computervision_client = ComputerVisionClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n",
        "\n",
        "# Enviar una solicitud para leer el texto escrito de la imagen y obtener el ID de operación\r\n",
        "read_operation = computervision_client.read_in_stream(image_stream,\n",
        "                                                      raw=True)\n",
        "operation_location = read_operation.headers[\"Operation-Location\"]\n",
        "operation_id = operation_location.split(\"/\")[-1]\n",
        "\n",
        "# Esperar hasta que se termine la operación asincrónica\r\n",
        "while True:\n",
        "    read_results = computervision_client.get_read_result(operation_id)\n",
        "    if read_results.status not in [OperationStatusCodes.running]:\n",
        "        break\n",
        "    time.sleep(1)\n",
        "\n",
        "# Una vez completada la operación, procesar el texto línea a línea\r\n",
        "if read_results.status == OperationStatusCodes.succeeded:\n",
        "    for result in read_results.analyze_result.read_results:\n",
        "        for line in result.lines:\n",
        "            print(line.text)\n",
        "\n",
        "# Abrir la imagen y mostrarla.\r\n",
        "print('\\n')\n",
        "fig = plt.figure(figsize=(12,12))\n",
        "img = Image.open(image_path)\n",
        "plt.axis('off')\n",
        "plt.imshow(img)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599694312346
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Revise los resultados. Encontrará una transcripción completa de la carta, que contiene en su mayoría texto impreso con una firma a mano. La imagen original de la carta aparece debajo de los resultados del OCR (quizá deba desplazarse hacia abajo para verla).\r\n",
        "\r\n",
        "## Leer texto manuscrito\r\n",
        "\r\n",
        "En el ejemplo anterior, la solicitud para analizar la imagen especificaba un modo de reconocimiento de texto que optimizaba la operación para texto *impreso*. A pesar de esto, se ha leído la firma manuscrita.\r\n",
        "\r\n",
        "Esta capacidad de leer texto manuscrito es muy útil. Por ejemplo, supongamos que ha escrito una nota con una lista de la compra y quiere usar una aplicación de su teléfono para leer la nota y transcribirla.\r\n",
        "\r\n",
        "Ejecute la siguiente celda para ver un ejemplo de una operación de lectura de una lista de la compra manuscrita."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
        "from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import time\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# Lectura del archivo de imagen\r\n",
        "image_path = os.path.join('data', 'ocr', 'note.jpg')\n",
        "image_stream = open(image_path, \"rb\")\n",
        "\n",
        "# Obtener un cliente del servicio Computer Vision\r\n",
        "computervision_client = ComputerVisionClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n",
        "\n",
        "# Enviar una solicitud para leer el texto escrito de la imagen y obtener el ID de operación\r\n",
        "read_operation = computervision_client.read_in_stream(image_stream,\n",
        "                                                      raw=True)\n",
        "operation_location = read_operation.headers[\"Operation-Location\"]\n",
        "operation_id = operation_location.split(\"/\")[-1]\n",
        "\n",
        "# Esperar hasta que se termine la operación asincrónica\r\n",
        "while True:\n",
        "    read_results = computervision_client.get_read_result(operation_id)\n",
        "    if read_results.status not in [OperationStatusCodes.running]:\n",
        "        break\n",
        "    time.sleep(1)\n",
        "\n",
        "# Una vez completada la operación, procesar el texto línea a línea\r\n",
        "if read_results.status == OperationStatusCodes.succeeded:\n",
        "    for result in read_results.analyze_result.read_results:\n",
        "        for line in result.lines:\n",
        "            print(line.text)\n",
        "\n",
        "# Abrir la imagen y mostrarla.\r\n",
        "print('\\n')\n",
        "fig = plt.figure(figsize=(12,12))\n",
        "img = Image.open(image_path)\n",
        "plt.axis('off')\n",
        "plt.imshow(img)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599694340593
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Más información\r\n",
        "\r\n",
        "Para obtener más información sobre cómo usar el servicio Computer Vision para realizar OCR, consulte la [documentación de Computer Vision](https://docs.microsoft.com/es-es/azure/cognitive-services/computer-vision/concept-recognizing-text)"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}