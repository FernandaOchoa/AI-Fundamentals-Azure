{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Detectar y analizar caras\r\n",
        "\r\n",
        "Las soluciones de Computer Vision suelen necesitar una solución de inteligencia artificial (IA) para poder detectar, analizar o identificar caras humanas. Por ejemplo: supongamos que la empresa Northwind Traders ha decidido implementar una tienda inteligente, en la que los servicios de IA supervisan la tienda para identificar a los clientes que necesiten ayuda y avisar a los empleados para que los ayuden. Una forma de conseguirlo es realizar procesos de detección y análisis facial, es decir, determinar si hay caras en las imágenes y, si es así, analizar sus características.\r\n",
        "\r\n",
        "![Un robot analizando una cara](./images/face_analysis.jpg)\r\n",
        "\r\n",
        "## Usar el servicio Face, de Cognitive Services, para detectar caras\r\n",
        "\r\n",
        "Supongamos que el sistema inteligente que Northwind Traders quiere crear necesita detectar a los clientes y analizar sus características faciales. En Microsoft Azure, puede usar **Face**, de Cognitive Services, para hacerlo.\r\n",
        "\r\n",
        "### Crear un recurso de Cognitive Services\r\n",
        "\r\n",
        "Para empezar, cree un recurso de **Cognitive Services** en su suscripción de Azure:\r\n",
        "\r\n",
        "> **Nota**: Si ya tiene un recurso de Cognitive Services, abra su página de **Inicio rápido** en Azure Portal y copie la clave y el punto de conexión en la siguiente celda. En caso contrario, siga estos pasos para crear uno.\r\n",
        "\r\n",
        "1. En la pestaña de otro explorador, abra Azure Portal (https://portal.azure.com) e inicie sesión con su cuenta de Microsoft.\r\n",
        "2. Haga clic en el botón **&#65291;Crear un recurso**, busque *Cognitive Services* y cree un recurso de **Cognitive Services** con esta configuración:\r\n",
        "    - **Suscripción**: *su suscripción de Azure*.\r\n",
        "    - **Grupo de recursos**: *seleccione o cree un grupo de recursos con un nombre único.*\r\n",
        "    - **Región**: *seleccione cualquier región disponible*:\r\n",
        "    - **Nombre**: *escriba un nombre único*.\r\n",
        "    - **Plan de tarifa**: S0\r\n",
        "    - **Confirmo que he leído y comprendido las notificaciones**: seleccionado.\r\n",
        "3. Espere a que la implementación finalice. Vaya al recurso de Cognitive Services y, en la página **Información general**, haga clic en el vínculo para administrar las claves del servicio. Necesitará el punto de conexión y las claves para conectarse a su recurso de Cognitive Services desde aplicaciones de cliente.\r\n",
        "\r\n",
        "### Obtener la clave y el punto de conexión de un recurso de Cognitive Services\r\n",
        "\r\n",
        "Para usar su recurso de Cognitive Services, las aplicaciones de cliente necesitan su clave de autenticación y su punto de conexión:\r\n",
        "\r\n",
        "1. En Azure Portal, en la página **Claves y punto de conexión** de su recurso de Cognitive Services, copie la **Key1** de su recurso y péguela en el siguiente código, en sustitución de **YOUR_COG_KEY**.\r\n",
        "\r\n",
        "2. Copie el **Punto de conexión** de su recurso y péguelo en el siguiente código, en sustitución de **YOUR_COG_ENDPOINT**.\r\n",
        "\r\n",
        "3. Haga clic en Run cell <span>&#9655;</span>, en la parte superior izquierda de la celda siguiente, para ejecutar su código."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "cog_key = 'YOUR_COG_KEY'\n",
        "cog_endpoint = 'YOUR_COG_ENDPOINT'\n",
        "\n",
        "print('Ready to use cognitive services at {} using key {}'.format(cog_endpoint, cog_key))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693964655
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora que tiene un recurso de Cognitive Services, puede usar el servicio Face para detectar caras humanas en la tienda.\r\n",
        "\r\n",
        "Ejecute la celda de código siguiente para ver un ejemplo."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.cognitiveservices.vision.face import FaceClient\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "from python_code import faces\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# Crear un cliente de detección facial.\r\n",
        "face_client = FaceClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n",
        "\n",
        "# Abrir una imagen\r\n",
        "image_path = os.path.join('data', 'face', 'store_cam2.jpg')\n",
        "image_stream = open(image_path, \"rb\")\n",
        "\n",
        "# Detectar caras\r\n",
        "detected_faces = face_client.face.detect_with_stream(image=image_stream)\n",
        "\n",
        "# Mostrar las caras (código en python_code/faces.py)\r\n",
        "faces.show_faces(image_path, detected_faces)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693970079
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se asignará un ID a cada cara detectada, para que la aplicación pueda identificar cada cara detectada.\r\n",
        "\r\n",
        "Ejecute la siguiente celda para ver los ID de la cara de algunos clientes."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Abrir una imagen\r\n",
        "image_path = os.path.join('data', 'face', 'store_cam3.jpg')\n",
        "image_stream = open(image_path, \"rb\")\n",
        "\n",
        "# Detectar caras\r\n",
        "detected_faces = face_client.face.detect_with_stream(image=image_stream)\n",
        "\n",
        "# Mostrar las caras (código en python_code/faces.py)\r\n",
        "faces.show_faces(image_path, detected_faces, show_id=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693970447
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analizar los atributos faciales\r\n",
        "\r\n",
        "Face puede ir mucho más allá del simple reconocimiento facial. También puede analizar las características y expresiones de las caras para indicar la edad y el estado emocional. Ejecute el siguiente código para analizar los atributos faciales de un cliente."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Abrir una imagen\r\n",
        "image_path = os.path.join('data', 'face', 'store_cam1.jpg')\n",
        "image_stream = open(image_path, \"rb\")\n",
        "\n",
        "# Detectar caras y determinados atributos faciales\r\n",
        "attributes = ['age', 'emotion']\n",
        "detected_faces = face_client.face.detect_with_stream(image=image_stream, return_face_attributes=attributes)\n",
        "\n",
        "# Mostrar las caras y los atributos (código en python_code/faces.py)\r\n",
        "faces.show_face_attributes(image_path, detected_faces)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693971321
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Según las puntuaciones de emoción detectadas para el cliente de la imagen, el cliente parece contento con su experiencia de compra.\r\n",
        "\r\n",
        "## Búsqueda de caras similares \r\n",
        "\r\n",
        "Los ID de caras creados para cada cara detectada se usan para identificar cada una de ellas. Puede usar estos ID para comparar una cara detectada con otras detectadas anteriormente y encontrar caras con características similares.\r\n",
        "\r\n",
        "Por ejemplo, ejecute la siguiente celda para comparar el cliente de una imagen con los clientes de otra y encontrar una cara que coincida."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener el identificador de la primera cara de la imagen 1\r\n",
        "image_1_path = os.path.join('data', 'face', 'store_cam3.jpg')\n",
        "image_1_stream = open(image_1_path, \"rb\")\n",
        "image_1_faces = face_client.face.detect_with_stream(image=image_1_stream)\n",
        "face_1 = image_1_faces[0]\n",
        "\n",
        "# Obtener los identificadores de caras en una segunda imagen\r\n",
        "image_2_path = os.path.join('data', 'face', 'store_cam2.jpg')\n",
        "image_2_stream = open(image_2_path, \"rb\")\n",
        "image_2_faces = face_client.face.detect_with_stream(image=image_2_stream)\n",
        "image_2_face_ids = list(map(lambda face: face.face_id, image_2_faces))\n",
        "\n",
        "# Encontrar las caras de la imagen 2 que sean similares a la de la imagen 1\r\n",
        "similar_faces = face_client.face.find_similar(face_id=face_1.face_id, face_ids=image_2_face_ids)\n",
        "\n",
        "# Mostrar la cara de la imagen 1 y las caras similares de la imagen 2 (código en python_code/face.py)\r\n",
        "faces.show_similar_faces(image_1_path, face_1, image_2_path, image_2_faces, similar_faces)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693972555
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reconocimiento de caras\r\n",
        "\r\n",
        "Hasta ahora, hemos visto que Face puede detectar caras y características faciales e identificar dos caras similares entre sí. Podemos ir más allá si implementamos una solución de *reconocimiento facial* en la que se entrene a Face para reconocer la cara de una persona en concreto. Esto puede ser útil en diferentes casos, como para etiquetar fotografías de amigos automáticamente en redes sociales o usar el reconocimiento facial como un sistema biométrico de verificación de identidad.\r\n",
        "\r\n",
        "Para ver cómo funciona, supongamos que Northwind Traders quiere usar el reconocimiento facial para garantizar que solo los empleados autorizados del departamento de TI pueda acceder a los sistemas seguros.\r\n",
        "\r\n",
        "Lo primero que haremos es crear un *grupo de personas* que represente a los empleados autorizados."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "group_id = 'employee_group_id'\n",
        "try:\n",
        "    # Elimine el grupo si ya existe\n",
        "    face_client.person_group.delete(group_id)\n",
        "except Exception as ex:\n",
        "    print(ex.message)\n",
        "finally:\n",
        "    face_client.person_group.create(group_id, 'employees')\n",
        "    print ('Group created!')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693973492
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez creado el *grupo de personas*, podemos agregar una *persona* para cada empleado que queramos incluir en el grupo y, después, agregar varias fotografías de cada persona para que Face pueda analizar las características faciales de cada persona. Lo ideal es que las imágenes muestren a la misma persona con diferentes poses y gestos.\r\n",
        "\r\n",
        "Agregaremos un solo empleado llamado Wendell y tres fotografías suyas."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# Agregar una persona (Wendell) al grupo\r\n",
        "wendell = face_client.person_group_person.create(group_id, 'Wendell')\n",
        "\n",
        "# Obtener una foto de Wendell\r\n",
        "folder = os.path.join('data', 'face', 'wendell')\n",
        "wendell_pics = os.listdir(folder)\n",
        "\n",
        "# Registrar las fotos\r\n",
        "i = 0\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "for pic in wendell_pics:\n",
        "    # Agregue cada foto a la persona en el grupo de personas\n",
        "    img_path = os.path.join(folder, pic)\n",
        "    img_stream = open(img_path, \"rb\")\n",
        "    face_client.person_group_person.add_face_from_stream(group_id, wendell.person_id, img_stream)\n",
        "\n",
        "    # Muestre cada imagen\n",
        "    img = Image.open(img_path)\n",
        "    i +=1\n",
        "    a=fig.add_subplot(1,len(wendell_pics), i)\n",
        "    a.axis('off')\n",
        "    imgplot = plt.imshow(img)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693976898
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez agregadas la persona y las fotografías, podemos entrenar a Face para que reconozca a Wendell."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "face_client.person_group.train(group_id)\n",
        "print('Trained!')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693977046
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez entrenado el modelo, podemos usarlo para identificar caras reconocidas en una imagen."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener los identificadores de caras en una segunda imagen\r\n",
        "image_path = os.path.join('data', 'face', 'employees.jpg')\n",
        "image_stream = open(image_path, \"rb\")\n",
        "image_faces = face_client.face.detect_with_stream(image=image_stream)\n",
        "image_face_ids = list(map(lambda face: face.face_id, image_faces))\n",
        "\n",
        "# Obtener los nombres de caras reconocidas\r\n",
        "face_names = {}\n",
        "recognized_faces = face_client.face.identify(image_face_ids, group_id)\n",
        "for face in recognized_faces:\n",
        "    person_name = face_client.person_group_person.get(group_id, face.candidates[0].person_id).name\n",
        "    face_names[face.face_id] = person_name\n",
        "\n",
        "# Mostrar las caras reconocidas\r\n",
        "faces.show_recognized_faces(image_path, image_faces, face_names)\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693994820
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Más información\r\n",
        "\r\n",
        "Para más información sobre el servicio Face, de Cognitive Services, consulte la [documentación de Face](https://docs.microsoft.com/azure/cognitive-services/face/).\r\n"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}