{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Language Understanding\r\n",
        "\r\n",
        "Cada día, aumentan las expectativas de que los equipos informáticos puedan usar la inteligencia artificial para comprender comandos escritos o hablados en lenguaje natural. Por ejemplo, cuando queremos implementar un sistema de automatización en el hogar que nos permita controlar dispositivos mediante comandos de voz como “enciende las luces” o “activa el ventilador” y que un dispositivo de inteligencia artificial pueda entender el comando y realizar la acción solicitada.\r\n",
        "\r\n",
        "![Un robot escuchando](./images/language_understanding.jpg)\r\n",
        "\r\n",
        "## Crear recursos de creación y predicción\r\n",
        "\r\n",
        "Microsoft Cognitive Services incluye el servicio Language Understanding, que permite definir *intenciones* aplicadas a *entidades* basadas en *expresiones*. \r\n",
        "\r\n",
        "Para usar el servicio Language Understanding, necesitará dos tipos de recursos:\r\n",
        "\r\n",
        "- Un recurso de *creación*: usado para definir, entrenar y probar el modelo de lenguaje. Este debe ser un recurso **Language Understanding - Authoring** en su suscripción de Azure.\r\n",
        "- Un recurso de *predicción*: se utiliza para publicar el modelo y administrar las solicitudes de aplicaciones de clientes que lo usan. Este puede ser un recurso de **Language Understanding** o de **Cognitive Services** en su suscripción de Azure.\r\n",
        "\r\n",
        "Puede usar un recurso de **Language Understanding** o de **Cognitive Services** para *publicar* una aplicación de Language Understanding, pero debe crear un recurso independiente de **Language Understanding** para *crear* la aplicación.\r\n",
        "\r\n",
        "> **Importante**: Los recursos de creación se deben generar en una de estas tres *regiones*: (Europa, Australia o EE. UU.). Los modelos creados en recursos de creación de Europa o Australia solo se pueden implementar en recursos de predicción de estas regiones. Los modelos creados en recursos de creación de EE. UU. se pueden implementar en recursos de predicción en cualquier ubicación de Azure que no sea Europa ni Australia. Consulte la [documentación sobre regiones de creación y publicación](https://docs.microsoft.com/azure/cognitive-services/luis/luis-reference-regions) para saber más sobre cómo seleccionar las ubicaciones de creación y predicción.\r\n",
        "\r\n",
        "1. En la pestaña de otro explorador, abra Azure Portal ([https://portal.azure.com](https://portal.azure.com)) e inicie sesión con su cuenta de Microsoft.\r\n",
        "2. Haga clic en **+ Crear un recurso** y busque *Language Understanding*.\r\n",
        "3. En la lista de servicios, haga clic en **Language Understanding**.\r\n",
        "4. En la hoja **Language Understanding**, haga clic en **Crear**.\r\n",
        "5. En la hoja **Crear**, escriba la siguiente información y haga clic en **Crear**:\r\n",
        "   - **Opción de creación**: ambas\r\n",
        "   - **Nombre**: *un nombre exclusivo para su servicio*\r\n",
        "   - **Suscripción**: *seleccione su suscripción de Azure*\r\n",
        "   - **Grupo de recursos**: *seleccione un grupo de recursos existente o cree uno nuevo*\r\n",
        "   - **Ubicación de creación**: *seleccione la ubicación que prefiera*\r\n",
        "   - **Plan de tarifa de creación**: F0\r\n",
        "   - **Ubicación de la predicción**: *seleccione una ubicación en la misma región de su ubicación de creación*\r\n",
        "   - **Plan de tarifa de predicción**: F0\r\n",
        "   \r\n",
        "6. Espere a que se creen los recursos, verá que se aprovisionan dos recursos de Language Understanding, uno para la creación y otro para la predicción. Para ver estos recursos, vaya al grupo de recursos en el que los creó.\r\n",
        "\r\n",
        "### Crear una aplicación de Language Understanding\r\n",
        "\r\n",
        "Para implementar la comprensión del lenguaje natural con Language Understanding, se debe crear una aplicación y luego agregar entidades, intenciones y expresiones para definir los comandos que la aplicación deba entender:\r\n",
        "\r\n",
        "1. En una nueva pestaña del navegador, abra el portal de Language Understanding de la región de creación en la que haya creado el recurso de creación:\r\n",
        "    - EE. UU.: [https://www.luis.ai](https://www.luis.ai)\r\n",
        "    - Europa: [https://eu.luis.ai](https://eu.luis.ai)\r\n",
        "    - Australia: [https://au.luis.ai](https://au.luis.ai)\r\n",
        "\r\n",
        "2. Inicie sesión con la cuenta de Microsoft asociada a su suscripción de Azure. Si es la primera vez que inicia sesión en el portal de Language Understanding, deberá conceder permisos a la aplicación para acceder a los detalles de su cuenta. Después, seleccione el recurso de creación de Language Understanding que ha creado con su suscripción de Azure y complete los pasos de *bienvenida*. \r\n",
        "\r\n",
        "3. Abra la página **Conversation Apps** y seleccione su suscripción y el recurso de creación de Language Understanding. Después, cree una nueva aplicación de conversación con la siguiente configuración:\r\n",
        "   - **Name**: Home Automation\r\n",
        "   - **Culture**: English (*si esta opción no está disponible, déjela en blanco*)\r\n",
        "   - **Description**: Simple home automation\r\n",
        "   - **Prediction resource**: *Su recurso de predicción de Language Understanding*\r\n",
        "\r\n",
        "4. Si aparece un panel con consejos para crear una aplicación de Language Understanding efectiva, ciérrelo.\r\n",
        "\r\n",
        "### Crear una entidad\r\n",
        "\r\n",
        "Una *entidad* es un componente que su modelo de lenguaje puede identificar y realizar acciones con él. En este caso, su aplicación de Language Understanding se usará para controlar varios *dispositivos* en la oficina, como las luces o los ventiladores. Por eso, crearemos una entidad llamada *device* que incluya una lista de los tipos de dispositivos que la aplicación deberá administrar. Para cada tipo de dispositivo, debe crear una sublista con el nombre del dispositivo, como *light*, y cualquier sinónimo que pueda asociarse a este tipo de dispositivo (como *lamp*).\r\n",
        "\r\n",
        "1. En la página de Language Understanding de su aplicación, en el panel izquierdo, haga clic en **Entities**. Después, haga clic en **Create** y cree una nueva entidad denominada **device**, seleccione el tipo **List** y haga clic en **Create**.\r\n",
        "2. En la página **List items**, en **Normalized Values**, escriba **light** y pulse ENTRAR.\r\n",
        "3. Una vez agregado el valor **light**, en **Synonyms**, escriba **lamp** y pulse ENTRAR.\r\n",
        "4. Agregue un segundo elemento de lista llamado **fan** con el sinónimo **AC**.\r\n",
        "\r\n",
        "> **Nota**: En este laboratorio, use las mayúsculas y minúsculas tal y como se indica _(por ejemplo: se debe usar light y **no** Light)_ y no agregue espacios adicionales. \r\n",
        "\r\n",
        "### Crear intenciones\r\n",
        "\r\n",
        "Una *intención* es una acción que quiere realizar en una o más entidades, por ejemplo, encender una luz o apagar un ventilador. En este caso, se definirán dos intenciones: una para activar el dispositivo y otra para apagarlo. Para cada intención, indicará *expresiones* de muestra que indiquen el tipo de lenguaje usado para marcar la intención.\r\n",
        "\r\n",
        "> **Nota**: En este laboratorio, use el texto con las mayúsculas y minúsculas tal y como aparecen _(por ejemplo: “turn the light on” y **no** “Turn the light on”)_ y no agregue espacios adicionales \r\n",
        "\r\n",
        "1. En el panel de la izquierda, haga clic en **Intents** Después, haga clic en **Create** y agregue una intención con el nombre **switch_on** y haga clic en **Done**.\r\n",
        "2. En el encabezado **Examples** y en el subencabezado **Example user input**, escriba la expresión ***turn the light on*** y pulse **Entrar** para agregar esta expresión a la lista.\r\n",
        "3. En la expresión *turn the light on*, haga clic en la palabra “light” y asígnela al valor **light** de la entidad **device**.\r\n",
        "\r\n",
        "![Asociar la palabra “light” al valor de la entidad.](./images/assign_entity.jpg)\r\n",
        "\r\n",
        "4. Agregue una segunda expresión a la intención **switch_on** con la frase ***turn the fan on***. Después, asocie la palabra “fan” al valor **fan** de la entidad **device**.\r\n",
        "5. En el panel izquierdo, haga clic en **Intents** y después en **Create** para agregar una segunda intención con el nombre **switch_off**.\r\n",
        "6. En la página **Utterances** de la intención **switch_off**, agregue la expresión ***turn the light off*** y asocie la palabra “light” al valor **light** de la entidad **device**.\r\n",
        "7. Agregue una segunda expresión a la intención **switch_off** con la frase ***turn the fan off***. Después, asocie la palabra “fan” al valor **fan** de la entidad **device**.\r\n",
        "\r\n",
        "### Entrenar y probar el modelo de lenguaje\r\n",
        "\r\n",
        "Ahora, ya puede usar los datos proporcionados en forma de entidades, intenciones y expresiones para entrenar el modelo de lenguaje de su aplicación.\r\n",
        "\r\n",
        "1. En la parte superior de la página de Language Understanding de su aplicación, haga clic en **Train** para entrenar el modelo de lenguaje\r\n",
        "2. Cuando el modelo se haya entrenado, haga clic en **Test** y use el panel de pruebas para ver la intención de la predicción de las siguientes frases:\r\n",
        "    * *switch the light on*\r\n",
        "    * *turn off the fan*\r\n",
        "    * *turn the lamp off*\r\n",
        "    * *switch on the AC*\r\n",
        "3. Cierre el panel de pruebas.\r\n",
        "    \r\n",
        "### Publicar el modelo y configurar puntos de conexión\r\n",
        "\r\n",
        "Para usar el modelo entrenado en una aplicación de cliente, debe publicarlo como un punto de conexión al que las aplicaciones de cliente puedan mandar nuevas expresiones, a partir de las cuales se predecirán las intenciones y entidades.\r\n",
        "\r\n",
        "1. En la parte superior de la página de Language Understanding de su aplicación, haga clic en **Publish**. Después, seleccione **Production slot** y haga clic en **Done**.\r\n",
        "\r\n",
        "2. Después de publicar el modelo, en la parte superior de la página de Language Understanding de su aplicación, haga clic en **Manage**. En la pestaña **Settings**, verá el **App ID** de su aplicación. Cópielo y péguelo en el código siguiente, en sustitución de **YOUR_LU_APP_ID**.\r\n",
        "\r\n",
        "3. En la pestaña **Azure Resources**, verá los valores de **Primary key** y **Endpoint URL** de su recurso de predicción. Copie estos valores y péguelos en el código, en sustitución de **YOUR_LU_KEY** y **YOUR_LU_ENDPOINT**, respectivamente.\r\n",
        "\r\n",
        "4. Ejecute la celda siguiente. Para ello, haga clic en el botón **Run cell** (&#9655;) (a la izquierda de la celda) y, cuando se le pida, escriba el texto *turn the light on*. El modelo de Language Understanding interpretará el texto y aparecerá la imagen adecuada.\r\n",
        "\r\n",
        "### **(!) Importante**: \r\n",
        "Busque el mensaje en la parte superior de la ventana. Deberá escribir *turn the light on* y pulsar **Entrar**. \r\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from python_code import luis\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "try:\n",
        "    # Establezca la configuración de la API\n",
        "    luis_app_id = 'YOUR_LU_APP_ID'\n",
        "    luis_key = 'YOUR_LU_KEY'\n",
        "    luis_endpoint = 'YOUR_LU_ENDPOINT'\n",
        "\n",
        "    # Solicite un comando\n",
        "    command = input('Please enter a command: \\n')\n",
        "\n",
        "    # Obtenga la intención y entidad previstas (el código está en python_code.home_auto.py)\n",
        "    action = luis.get_intent(luis_app_id, luis_key, luis_endpoint, command)\n",
        "\n",
        "    # Muestre una imagen adecuada\n",
        "    img_name = action + '.jpg'\n",
        "    img = Image.open(os.path.join(\"data\", \"luis\" ,img_name))\n",
        "    plt.axis('off')\n",
        "    plt. imshow(img)\n",
        "except Exception as ex:\n",
        "    print(ex)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [],
        "gather": {
          "logged": 1599696381331
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (!) Comprobar \r\n",
        "¿Ha ejecutado la celda anterior y ha escrito la frase *turn the light on* cuando se le ha pedido? El mensaje para escribir la frase debe aparecer en la parte superior de su ventana.  \r\n",
        "\r\n",
        "Vuelva a ejecutar la celda anterior con estas frases:\r\n",
        "\r\n",
        "* *turn on the light*\r\n",
        "* *put the lamp off*\r\n",
        "* *switch the fan on*\r\n",
        "* *switch the light on*\r\n",
        "* *switch off the light*\r\n",
        "* *turn off the fan*\r\n",
        "* *switch the AC on*\r\n",
        "\r\n",
        "Si ejecuta la celda anterior y aparece una imagen con un símbolo de interrogación, es posible que haya utilizado un texto diferente o que haya agregado espacios incorrectos al crear la entidad, la intención o la expresión.\r\n",
        "\r\n",
        "> **Nota**: Si quiere conocer el código utilizado para recuperar las intenciones y entidades de su aplicación de Language Understanding, busque el archivo **luis.py** en la carpeta **python_code**."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agregar control de voz\r\n",
        "\r\n",
        "Hasta ahora, hemos visto cómo analizar texto. Sin embargo, cada día es más habitual que los sistemas de inteligencia artificial permitan a los humanos comunicarse con servicios de software mediante el reconocimiento de voz. Para conseguirlo, el servicio **Voz**, parte de Cognitive Services, proporciona una forma sencilla de transcribir un mensaje hablado en texto.\r\n",
        "\r\n",
        "### Crear un recurso de Cognitive Services\r\n",
        "\r\n",
        "Si no tiene uno, siga estos pasos para crear un recurso de **Cognitive Services** en su suscripción de Azure:\r\n",
        "\r\n",
        "> **Nota**: Si ya tiene un recurso de Cognitive Services, abra su página de **Inicio rápido** en Azure Portal y copie la clave y ubicación en la siguiente celda. En caso contrario, siga estos pasos para crear uno.\r\n",
        "\r\n",
        "1. En la pestaña de otro explorador, abra Azure Portal ([https://portal.azure.com](https://portal.azure.com)) e inicie sesión con su cuenta de Microsoft.\r\n",
        "2. Haga clic en el botón **&#65291;Crear un recurso**, busque *Cognitive Services* y cree un recurso de **Cognitive Services** con esta configuración:\r\n",
        "    - **Suscripción**: *su suscripción de Azure*.\r\n",
        "    - **Grupo de recursos**: *seleccione o cree un grupo de recursos con un nombre único.*\r\n",
        "    - **Región**: *seleccione cualquier región disponible*:\r\n",
        "    - **Nombre**: *escriba un nombre único*.\r\n",
        "    - **Plan de tarifa**: S0\r\n",
        "    - **Al marcar esta casilla, afirmo que este servicio no está dirigido a un departamento de policía de Estados Unidos**: seleccionado.\r\n",
        "    - **Confirmo que he leído y comprendido las notificaciones**: seleccionado.\r\n",
        "3. Espere a que la implementación finalice. Después, vaya al recurso de Cognitive Services y, en la página **Inicio rápido**, anote las claves y la ubicación. Las necesitará para conectarse al recurso de Cognitive Services desde aplicaciones de cliente.\r\n",
        "\r\n",
        "### Obtener la clave y la ubicación de un recurso de Cognitive Services\r\n",
        "\r\n",
        "Para usar su recurso de Cognitive Services, las aplicaciones de cliente necesitan su clave de autenticación y su ubicación:\r\n",
        "\r\n",
        "1. En Azure Portal, en la página **Claves y punto de conexión** de su recurso de Cognitive Services, copie la **Key1** de su recurso y péguela en el siguiente código, en sustitución de **YOUR_COG_KEY**.\r\n",
        "2. Copie la **Ubicación** de su recurso y péguela en el siguiente código, en sustitución de **YOUR_COG_LOCATION**.\r\n",
        ">**Nota**: Quédese en la página **Claves y punto de conexión** y copie la **Ubicación** desde esta página, (ejemplo: _westus_). No _agregue_ espacios entre las palabras del campo Ubicación. \r\n",
        "3. Ejecute el código de la celda siguiente. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "cog_key = 'YOUR_COG_KEY'\n",
        "cog_location = 'YOUR_COG_LOCATION'\n",
        "\n",
        "print('Ready to use cognitive services in {} using key {}'.format(cog_location, cog_key))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [],
        "gather": {
          "logged": 1599696409914
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, ejecute la celda siguiente para transcribir la voz de un archivo de audio y usarla como un comando para su aplicación de Language Understanding."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from python_code import luis\n",
        "from azure.cognitiveservices.speech import SpeechConfig, SpeechRecognizer, AudioConfig\n",
        "from playsound import playsound\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "\n",
        "try:   \n",
        "\n",
        "    # Obtenga el comando hablado del archivo de audio\n",
        "    file_name = 'light-on.wav'\n",
        "    audio_file = os.path.join('data', 'luis', file_name)\n",
        "\n",
        "    # Configure el reconocedor de voz\n",
        "    speech_config = SpeechConfig(cog_key, cog_location)\n",
        "    audio_config = AudioConfig(filename=audio_file) # Utilice el archivo en lugar del valor predeterminado (micrófono)\n",
        "    speech_recognizer = SpeechRecognizer(speech_config, audio_config)\n",
        "\n",
        "    # Use una llamada sincrónica única para transcribir la voz\n",
        "    speech = speech_recognizer.recognize_once()\n",
        "\n",
        "    # Obtenga la intención y entidad previstas (el código está en python_code.home_auto.py)\n",
        "    action = luis.get_intent(luis_app_id, luis_key, luis_endpoint, speech.text)\n",
        "\n",
        "    # Obtenga la imagen adecuada\n",
        "    img_name = action + '.jpg'\n",
        "\n",
        "    # Reproduzca el audio \n",
        "    playsound(audio_file)\n",
        "\n",
        "    # Muestre la imagen \n",
        "    img = Image.open(os.path.join(\"data\", \"luis\" ,img_name))\n",
        "    plt.axis('off')\n",
        "    plt. imshow(img)\n",
        "\n",
        "except Exception as ex:\n",
        "    print(ex)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [],
        "gather": {
          "logged": 1599696420498
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Edite la celda anterior para que use el archivo de audio **light-off.wav**.\r\n",
        "\r\n",
        "## Más información\r\n",
        "\r\n",
        "Obtenga más información sobre Language Understanding en la [documentación del servicio](https://docs.microsoft.com/azure/cognitive-services/luis/)"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}